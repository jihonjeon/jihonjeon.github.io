---
title: "IEEE 754: float vs double ?"
date: 2025-07-18 12:00:00 +0900
categories: [ë¸”ë¡œê·¸, ì²«ê¸€]
tags: [IEEE754, float, double, kahan]
math: true  # ðŸ‘ˆ ìˆ˜ì‹ ë Œë”ë§ìš© ì„¤ì •
---

# IEEE 754: float vs double ?

## 1. Intro

Before the IEEE 754 standard, floating-point number representations varied across manufacturers.  
This caused inconsistencies in numerical computations across systems.   
To address this issue, a standard was proposed in collaboration with hardware companies like Intel and Motorola and numerical analysts such as **William Kahan** who is now a prominent figure in this field. Before diving into precision algorithms, we should first cover the IEEEâ€¯754 floatingâ€‘point representation--including both normal and subnormal numbers--
and various error modes inherent in IEEE 754 floating-point systems, such as rounding error and overflow.

- What is IEEE 754?

The IEEE 754 standard defines:

- Binary formats ( e.g., float = 32 bits, double = 64 bits, etc.)
- Layout of sign / exponent / mantissa (fraction)
- Rules for rounding, overflow, underflow, NaNs, and infinities

It became the **default representation** for floating-point arithmetic in most modern computing systems.


IEEE 754 represents numbers in the form:

$$
x = (-1)^s \times (1.f) \times 2^{e - \text{bias}} 
$$

the value of bias depends on whether it is single or double precision. 
if it is double precision, then the bias is 1023; otherwise, it's 127.



| Component  | Description                        |
|------------|------------------------------------|
| Sign bit   | 0 for positive, 1 for negative     |
| Exponent   | Encodes magnitude (biased format)  |
| Mantissa   | Fractional bits after binary point |



## 4. Float vs Double: Bit-level Differences

IEEE 754 defines the formats as follows:

| Type   | Total Bits | Sign | Exponent | Mantissa |
|--------|------------|------|----------|----------|
| float  | 32         | 1    | 8        | 23       |
| double | 64         | 1    | 11       | 52       |

Mathematically:

- Float:
  $$
  x = \pm (1.a_1 a_2 \dots a_{23}) \times 2^{E-bias}
  $$
- Double:
  $$
  x = \pm (1.a_1 a_2 \dots a_{52}) \times 2^{E-bias}
  $$
  
---

## 3. Precision and Rounding Errors

Floating-point operations are not associative due to rounding:

```python
# Example in Python
a = 1e20
b = -1e20
c = 3.14

print((a + b) + c)  # prints 3.14
print(a + (b + c))  # prints 0.0
```

## 4. Precision Algorithms 

- Kahan Two Sum
 This algorithm can guarantee $O(n\epsilon)$ about n summation terms.
 Let's think of this why.

```python

def KahanTwoSum(x:[list:float]):
  c = 0.0 # current error 
  t = 0.0 # global sum 
  for x_ele in x:
    y = x_ele - c
    total = t + y # an error might exists + e1 
    c = (total - t) - y
    t = total 
```

- Neumaier (1974)
  This is invented for addressing the error gap when using Kahan Two Sum.

```python 

def Neumaier(x:[list:float]):
  c = 0.0
  t = 0.0
  for x_ele in x:
    total = t + x_ele # f(t + x_ele) = t + x_ele + $\epsilon$
    if abs(total) >= abs(x_ele):
      c += (t - total) - x_ele  
    else:
      c += (x_ele - total) + t
    t = total
  return t + c
```

- Superaccumulator

    
```python

import struct 

def superacc(x_list[list:float]):
  
    
```

and there are much more sophisticated algorithms divided into two ways; by software or hardware. 


