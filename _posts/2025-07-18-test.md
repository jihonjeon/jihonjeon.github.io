---
title: "IEEE 754: float vs double ?"
date: 2025-07-18 12:00:00 +0900
categories: [Î∏îÎ°úÍ∑∏, Ï≤´Í∏Ä]
tags: [IEEE754, float, double, kahan]
math: true  # üëà ÏàòÏãù Î†åÎçîÎßÅÏö© ÏÑ§Ï†ï
---

# IEEE 754: float vs double ?

## 1. Intro

Before the IEEE 754 standard, floating-point number representations varied across manufacturers.  
This caused inconsistencies in numerical computations across systems.   
To address this issue, a standard was proposed in collaboration with hardware companies like Intel and Motorola and numerical analysts such as **William Kahan** who is now a prominent figure in this field.

---

## 2. What is IEEE 754?

The IEEE 754 standard defines:

- Binary formats ( e.g., float = 32 bits, double = 64 bits, etc.)
- Layout of sign / exponent / mantissa (fraction)
- Rules for rounding, overflow, underflow, NaNs, and infinities

It became the **default representation** for floating-point arithmetic in most modern computing systems.

---

## 3. Internal Structure

IEEE 754 represents numbers in the form:

$$
x = (-1)^s \times (1.f) \times 2^{e - \text{bias}} 
$$

the value of bias depends on whether it is single or double precision. 
if it is double precision, then the bias is 1023; otherwise, it's 127.



| Component  | Description                        |
|------------|------------------------------------|
| Sign bit   | 0 for positive, 1 for negative     |
| Exponent   | Encodes magnitude (biased format)  |
| Mantissa   | Fractional bits after binary point |

For example:

- **float (32-bit)**: 1 sign + 8 exponent + 23 mantissa
- **double (64-bit)**: 1 sign + 11 exponent + 52 mantissa

---

## 4. Float vs Double: Bit-level Differences

IEEE 754 defines the formats as follows:

| Type   | Total Bits | Sign | Exponent | Mantissa |
|--------|------------|------|----------|----------|
| float  | 32         | 1    | 8        | 23       |
| double | 64         | 1    | 11       | 52       |

Mathematically:

- Float:
  $$
  x = \pm (1.a_1 a_2 \dots a_{23}) \times 2^{E}
  $$
- Double:
  $$
  x = \pm (1.a_1 a_2 \dots a_{52}) \times 2^{E}
  $$
  where E = E - bias 
---

## 5. Precision and Rounding Errors

Floating-point operations are not associative due to rounding:

```python
# Example in Python
a = 1e20
b = -1e20
c = 3.14

print((a + b) + c)  # prints 3.14
print(a + (b + c))  # prints 0.0
```

## 6. Algorithms 

- Kahan Two Sum

- 
